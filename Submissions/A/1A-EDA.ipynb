{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import geopandas as gpd #having some dificulty with geopandas, have tried a number of different versions,\n",
    "#appears pyproj is the issue\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "from sklearn.neighbors import KNNClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imported from Github repo, also available at the following source as provide by DrivenData on behalf of Taarifa (Tanzania Ministry of Water)\n",
    "\n",
    "Data Source <https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/data/>\n",
    "\n",
    "Data inlcudes a set of training data with labels and a set of test data for prediction and submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the data below gives us a feel for what has been provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pd.read_csv('Source_data/trainset_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that ther are some null values in columns \n",
    "['funder', 'installer', 'subvillage', 'public_meeting', 'scheme_management', 'scheme_name', 'permit']\n",
    "that will need to be handled. It also appears that some columns provide overlapping information (extraction, management, payment, water quality and quantity, source and waterpoint). We will have to investiagte further if each column provides enough new information to keep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv('Source_data/trainset_labels.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the training labels below we see that there are 3 groups: functional, non functional and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=classes, x='status_group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- amount_tsh - Total static head (amount water available to waterpoint)\n",
    "- date_recorded - The date the row was entered\n",
    "- funder - Who funded the well\n",
    "- gps_height - Altitude of the well\n",
    "- installer - Organization that installed the well\n",
    "- longitude - GPS coordinate\n",
    "- latitude - GPS coordinate\n",
    "- wpt_name - Name of the waterpoint if there is one\n",
    "- num_private -\n",
    "- basin - Geographic water basin\n",
    "- subvillage - Geographic location\n",
    "- region - Geographic location\n",
    "- region_code - Geographic location (coded)\n",
    "- district_code - Geographic location (coded)\n",
    "- lga - Geographic location\n",
    "- ward - Geographic location\n",
    "- population - Population around the well\n",
    "- public_meeting - True/False\n",
    "- recorded_by - Group entering this row of data\n",
    "- scheme_management - Who operates the waterpoint\n",
    "- scheme_name - Who operates the waterpoint\n",
    "- permit - If the waterpoint is permitted\n",
    "- construction_year - Year the waterpoint was constructed\n",
    "- extraction_type - The kind of extraction the waterpoint uses\n",
    "- extraction_type_group - The kind of extraction the waterpoint uses\n",
    "- extraction_type_class - The kind of extraction the waterpoint uses\n",
    "- management - How the waterpoint is managed\n",
    "- management_group - How the waterpoint is managed\n",
    "- payment - What the water costs\n",
    "- payment_type - What the water costs\n",
    "- water_quality - The quality of the water\n",
    "- quality_group - The quality of the water\n",
    "- quantity - The quantity of water\n",
    "- quantity_group - The quantity of water\n",
    "- source - The source of the water\n",
    "- source_type - The source of the water\n",
    "- source_class - The source of the water\n",
    "- waterpoint_type - The kind of waterpoint\n",
    "- waterpoint_type_group - The kind of waterpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the labels onto the values dataframe in order to do some further EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.insert(loc=1, column='class', value=classes['status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some details of the source columns ['source', 'source_type', 'source_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values['source'].value_counts(), values['source_type'].value_counts(), values['source_class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'source' and 'source_type' have very little difference expect:\n",
    "- 'river' and 'lake' categories combined to 'river/lake'\n",
    "- 'other' and 'unknown' combined into a single category called 'other'\n",
    "- 'machine dbw' and 'hand dtw' were combined into a single 'borehole' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the distribution of the labels overall\n",
    "labelcount = values.groupby('class')['id'].count()\n",
    "labelcount = pd.concat([labelcount, values.groupby('class')['id'].apply(lambda x:'{0:.1f}%'.format(x.count()/len(values['id'])*100))], axis=1)\n",
    "labelcount.columns = ['count', 'percent']\n",
    "labelcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare this distrubtion against the distribution with each source type. If the value for the different sources are consistent with the average then it would mean we aren't necessary gather more data from having the most robust amount of columns. However, if there are differences then we would want to keep those differences intact for our model. We are looking to choose which of the source columns to use in order to limit colinearity in our data, though we could add this information back in at a later time if we think it could increase our model perfomance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviewing counts of labels per each source\n",
    "sourcecount = values.groupby(['class', 'source'])['id'].count().unstack()\n",
    "sourcecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USing our counts to determine percentges for simpler comparison against the total percentages found above\n",
    "sourceper = sourcecount/sourcecount.sum()*100\n",
    "sourceper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be doing multiple comparisons between the categorical label values and the other categorical columns, lets create a new fuction that create a clean and informational visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chi_sq_test(df, dependant, independant):\n",
    "    #takes in the names of a dependant and independant variable (column), runs a chi squared test and then outputs \n",
    "    #a seaborn heatmap of the percent difference between the expected and actual values\n",
    "    \n",
    "    #create cotingency table\n",
    "    count_table = df.groupby([dependant, independant])['id'].count().unstack()\n",
    "    count_table.fillna(0, inplace=True)\n",
    "    count_table = count_table.astype('int')\n",
    "    \n",
    "    #Chi Squared test is for only counts above 5, we are keeping the same ratio, but increasing min value to 5 in each column\n",
    "    if count_table.isin(range(0,5)).any().any():\n",
    "        for j in range(len(count_table.columns)):\n",
    "            for i in range(len(count_table.index)):\n",
    "                if count_table.iloc[i,j] < 1:\n",
    "                    count_table.iloc[i,j] = 5\n",
    "                    count_table.iloc[:,j] = count_table.iloc[:,j]*5\n",
    "                elif count_table.iloc[i,j] <5:\n",
    "                    count_table.iloc[:,j] = count_table.iloc[:,j]*(5/count_table.iloc[i,j])\n",
    "    \n",
    "    stat, p, dof, expected = chi2_contingency(count_table)\n",
    "    \n",
    "    #print test information\n",
    "    print('P-Value = {}'.format(p))\n",
    "    print('Chi Statistic = {}'.format(stat))\n",
    "    print('Degrees of Freedom = {}'.format(dof))\n",
    "    \n",
    "    #caluclate and print heatmap\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.heatmap(((count_table - expected) / count_table *100), annot=True, vmax=100, vmin=-100, fmt='.1f', \n",
    "                annot_kws={'rotation': 90}, cmap='viridis')\n",
    "    plt.title('Percent Difference of Expected vs. Actual Classes per {}'.format(str.title(independant)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(values, 'class', 'source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall data had values of:\n",
    "- functional 54.3%\n",
    "- functional needs repair 7.3% \n",
    "- non functional 38.4%\n",
    "\n",
    "Source categories hand dtw, other, river, shallow well, unknown\n",
    "\n",
    "Source categories dam, lake, machine dbh, rainwater harvesting, and spring deviate from the overall numbers in varying ways. For example, dam had nearly a reversal of the percentages for functional and non functional making it a strong feature for recognizing non functional well points. Also while rainwater harvesting has a slightly higher to overall percent functional, it also has double the functional needs repair of the overall and will be a good indicator for the that category. We will keep the 'source' column and drop 'source_type' and 'source_class'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = values.drop(columns=['source_type', 'source_class'])\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting source and label comparisons\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax=sns.histplot(data=values, x='source', hue='class', multiple='dodge')\n",
    "total = len(values['class'])\n",
    "for p in ax.patches:\n",
    "    height=p.get_height()\n",
    "    ax.text(x=p.get_x()+(p.get_width()/2), y=height+0., s=height, ha='center')\n",
    "plt.title('Class vs Source')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the other features that have multiple columns. For now we are looking to keep only one of each column to limit collinearity, though we can come back later if we feel they have value to add. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values['extraction_type'].value_counts(), values['extraction_type_group'].value_counts(), values['extraction_type_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'extraction_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall data had values of:\n",
    "- functional 54.3%\n",
    "- functional needs repair 7.3% \n",
    "- non functional 38.4%\n",
    "\n",
    "Most categories seem to deviate from the norm so we want to keep ans many as possible, however some have relatively small data asscoaited with them, i.e. other-mkulima/shinyanga. For that reason we are going to use the intermediate break down with the exception of keeping ksb and submersible seperate as well as cemo and climax seperate. These categories that are merged under the extraction_type_group column seem to have enough value to keep seperate. We will keep the extraction_type column, but map other swn 81, other-play pump, walimi and other-mkulima/shinyanga together into a single other-handpump group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['extraction_type_group', 'extraction_type_class'])\n",
    "data['extraction_type'].replace({'other - swn 81':'other-handpump',\n",
    "                                 'other - play pump':'other-handpump', \n",
    "                                 'walimi':'other-handpump', \n",
    "                                 'other - mkulima/shinyanga':'other-handpump',\n",
    "                                'swn 80':'swn_80',\n",
    "                                 'nira/tanira':'nira-tanira',\n",
    "                                'india mark ii':'india_mark_ii',\n",
    "                                'india mark iii':'india_mark_iii',\n",
    "                                'other - rope pump':'other-rope_pump',}, inplace=True)\n",
    "data['extraction_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'extraction_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also clean up some of the source names before we go further to eliminate erroneous spaces and symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['source'].replace({'shallow well':'shallow_well',\n",
    "                       'machine dbh':'machine_dbh',\n",
    "                       'rainwater harvesting':'rainwater_harvesting',\n",
    "                       'hand dtw':'hand_dtw'}, inplace=True)\n",
    "data['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting extraction_type and label comparisons\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax=sns.histplot(data=values, x='extraction_type', hue='class', multiple='dodge')\n",
    "total = len(values['class'])\n",
    "for p in ax.patches:\n",
    "    height=p.get_height()\n",
    "    ax.text(x=p.get_x()+(p.get_width()/2), y=height+0., s=height, ha='center')\n",
    "plt.title('Class vs Extraction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at some of the location columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "data[['region', 'region_code', 'subvillage']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 19k subvillages. It would be better to take the more common villages and group the less common villages into an 'other' categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subvillage'].value_counts().head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of subvillages that are not names. We will need to find them and correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svshort = data.loc[data['subvillage'].str.len() <=3]\n",
    "svshort['subvillage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svshort['subvillage'].nunique(), data['subvillage'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the amount of subvillages that don't have proper names and the amount of categories, we are going to group all of the subvillages with less than 200 well points into a single category of other. This allows any predictive power form the areas with more wells, which we would think are more populous and possibly more used/serviced, to remain without creating noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['subvillage'].value_counts()\n",
    "counts = counts.loc[counts >=200]\n",
    "counts = list(counts.index)\n",
    "data.loc[~data['subvillage'].isin(counts), 'subvillage'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subvillage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'subvillage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['region_code'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the region column and keep the region code column as region code has more categories. Without further outside research we can't determine the match up between the regions and the region codes and therefore can't confirm there collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'region_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['region'], inplace=True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some null values in the dataframe. Based upon the value being in categorical columns, witrh the exception of public meeting and permit, we will fill in the value 'unknown'. For permit and public meeting we will fill in False, making the assumption that the data not being recorded makes it less likely the additoinal administrative steps were taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(inplace=True, value={'installer':'unknown','permit':False, 'funder':'unknown', 'public_meeting':False, \n",
    "                                 'scheme_management':'unknown', 'scheme_name':'unknown'})\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a further look at some of the geographic data in column lga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lga'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lga'].str.contains('Urban|urban|Rural|rural').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did with subvillage, we will keep only the categories with large numbers of well points. Since only 1 category besides other has more than 200 well points we will modify this into a binary column for lga_Njombe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lga_Njombe'] = data['lga'].replace({'Njombe':1})\n",
    "data.loc[data['lga_Njombe']!=1, 'lga_Njombe'] = 0\n",
    "data['lga_Njombe'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['lga'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'lga_Njombe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the payment infomration in columns payemnt and payment_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['payment'].value_counts(), values['payment_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the two columns were recording the same values, but that the payment_type column has many more non-other entries. We will drop the payment column to avoid collinearity.\n",
    "\n",
    "Now we will look at the basin column to see if it needs any cleaning/modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['payment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'payment_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['basin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will leave the categories un altered as it seems to have a good distribution, but we will modify some of the strings to eliminate the /'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['basin'].replace({'Ruvuma / Southern Coast':'Ruvuma-Southern_Coast',\n",
    "                     'Wami / Ruvu':'Wami-Ruvu'}, inplace=True)\n",
    "data['basin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'basin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' look into the dates recorded column to see if there are any trends. We will start with tranforming it into a datetime object and then extract the month and year into seperate columns for simpler viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_recorded']= pd.to_datetime(values['date_recorded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_recorded'].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_recorded'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year']=data['date_recorded'].dt.year\n",
    "data['month']=data['date_recorded'].dt.month\n",
    "data[['month', 'year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before let's take a look at the percentages for each label by month to see how it matches up with the overall percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "ax=sns.histplot(data=data, x='month', hue='class', multiple='dodge')\n",
    "for p in ax.patches:\n",
    "    height=p.get_height()\n",
    "    ax.text(x=p.get_x()+(p.get_width()/2)+.05, y=height+0.25, s=height, ha='center')\n",
    "plt.title('Month of Year vs. Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a some seasonality to when inspections are performed. This possibly coincides with wet and dry seasons in the region. Persons managing the wellpoints would typically have additional inspections before and during times of use to ensure limited down time during the peak demand. It would be expected that all types of wellpoints would have better water access during a wet season as groundwater levels rise, though we don't know how this becomes associated with demand as more non-wellpoint bodies of water are likely available at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviewing counts of labels per each month\n",
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does appear to be a difference month to month in level of functioning equipment. Let's take a look further at a single year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "ax=sns.histplot(data=data.loc[data['year']>2010], x='year', hue='class', multiple='dodge')\n",
    "for p in ax.patches:\n",
    "    height=p.get_height()\n",
    "    ax.text(x=p.get_x()+(p.get_width()/2)+.05, y=height+0.25, s=height, ha='center')\n",
    "plt.xticks([2011, 2012, 2013])\n",
    "plt.title('Class Distribution per Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2011 and 2013 have simliar distributions of each class. 2012 appears to have a higher percentage of non functional class but we don't know if that was more a by product of the samller smaple size of inspections. Without knowing the reason for the considerable drop in inspections, we can't confidently say that there was a different distribution of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviewing counts of labels per each year\n",
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop a few additional columns that don't give value to our predictive power. wpt_name, num_private and recorded_by are all arbitrary informatio that we don't expect to gain insight from. They will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['wpt_name', 'num_private', 'recorded_by'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the the permit and public meeting columns to be 0 and 1 instead of True and False to make them easier for model ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['public_meeting'] = data['public_meeting'].map({True:1, False:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'public_meeting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['permit'] = data['permit'].map({True:1, False:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'permit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the district code column and see if it differs greatly from the region_code column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['district_code'].nunique(), data['district_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['district_code']==data['region_code']]['region_code'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'district_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on there being limited times where the district_code is the same as the region_code and that we don't have a better definition for either variable, we will keep both\n",
    "\n",
    "Let's continue by looking at construction_year and transforming it into a years old column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.histplot(data=data, x='construction_year', hue='class', multiple='dodge', bins=20)\n",
    "plt.title('Class Distribution per Construction Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['construction_year'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a float and seems to have many zeros we will have to make some corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at how the data is spread out\n",
    "data['construction_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking just at the non zero data\n",
    "data.loc[data['construction_year']>0,'construction_year'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 1999 as a fill in date for the zeroes. This is based on the nonzero entries having a mean of ~1997 and a median of 2000. We will also convert it to a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['construction_year'] = values['construction_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['construction_year'].replace({0:1999}, inplace=True)\n",
    "data['construction_year'] = pd.to_datetime(data['construction_year'], format='%Y')\n",
    "data['construction_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['years_old'] = data['date_recorded'].dt.year - data['construction_year'].dt.year\n",
    "data['years_old']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['years_old'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['years_old']<0, 'years_old'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.histplot(data=data, x='years_old', hue='class', multiple='dodge', bins=20)\n",
    "plt.title('Class Distribution per Construction Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at quality, quantity and waterpoint type. As with the previous source and extraction columns we are looking to limit collinearity without giving away data that is useful for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['water_quality'].value_counts(), data['quality_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['quantity'].value_counts(), data['quantity_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quality, it would seem the only change from water quality to quality group is combining salty columns, fluoride columns and renaming soft to good. We will drop the quality group column. \n",
    "\n",
    "For quantity, there is diference between the columns so we will drop the quantity group column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['quality_group', 'quantity_group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'water_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['waterpoint_type'].value_counts(), data['waterpoint_type_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference being the combination of communal standpipe columns in waterpoint type group. We will drop the waterpoint type group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['waterpoint_type_group'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'waterpoint_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the management data. It is likely we will follow the same logic as with the subvillages where we keep the larger groups and part the smaller ones into bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scheme_management'].value_counts(), data['scheme_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two scheme columns have different enough infomration to keep both. The scheme management column with be left alone except for moving the 1 'None' value into the 'other' category. The scheme name column will need to have it's categories paired down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scheme_name'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts2 = data['scheme_name'].value_counts()\n",
    "counts2 = counts2.loc[counts2 >=200]\n",
    "counts2 = list(counts2.index)\n",
    "data.loc[~data['scheme_name'].isin(counts2), 'scheme_name'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['scheme_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'scheme_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['management'].value_counts(), data['management_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep both columns unchanged. Let's take a look into the installer and funder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['funder'].value_counts(), data['installer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our previous categorical columns, there are a lot of categpories that have very small counts. We will compbine them into a single other category and keep the categories with larger counts seperated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['funder'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our previous categorical columns, there are a lot of categpories that have relatively small counts. We will combine them into a single other category and keep the categories with larger counts seperated. The number for seperation is a guess for the time being and we can come back and adjust it if we find this to be an important feature to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts3 = data['funder'].value_counts()\n",
    "counts3 = counts3.loc[counts3 >=500]\n",
    "counts3 = list(counts3.index)\n",
    "data.loc[~data['funder'].isin(counts3), 'funder'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['funder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'funder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['installer'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our previous categorical columns, there are a lot of categpories that have relatively small counts. We will combine them into a single other category and keep the categories with larger counts seperated. The number for seperation is a guess for the time being and we can come back and adjust it if we find this to be an important feature to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts4 = data['installer'].value_counts()\n",
    "counts4 = counts4.loc[counts4 >=500]\n",
    "counts4 = list(counts4.index)\n",
    "data.loc[~data['installer'].isin(counts4), 'installer'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['installer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'installer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are curious to see if popultion plays a role in the distribution of classes. More population likely means more demand, but also could mean more resources in the area to fix problems more quickly and/or maintaint the wellpoint more frequently allowing for less issues leading the wellpoint being non functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['population'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.histplot(data=data, x='population', hue='class', bins=10, multiple='dodge')\n",
    "plt.title('Class Distribution vs. Population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the long right tail of the data it is difficult to see where the distribution of the lower end data whihc includes the majority of the data. We can look at only areas with populations between 2 and 2000. This will also eliminate the assumedly erroneaus entries of 0 and 1 for population from the visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['population'].between(2, 2000), 'population'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.histplot(data=data.loc[data['population'].between(2,2000)], x='population', hue='class', bins=10, multiple='dodge')\n",
    "plt.title('Class Distribution per Population, Population <= 2000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new bin column for population with additional bins at the lower end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['popbins'] = pd.cut(data['population'], [-1,2,250,500,1000,2500,10000,40000], labels=list(range(1,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['popbins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'popbins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the final categorical features: ward. We can check if it matches up with the subvillage column in which case we will simply drop it to avoid colinearity. If not it is likely to be another column with many categories that will need to have categories with smaller counts combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ward'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['ward']==data['subvillage']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ward'].value_counts().plot()\n",
    "plt.title('Counts of Ward Categories')\n",
    "plt.xlabel('Ward')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can break the wards in five categories: verybig, big, medium, small and verysmall. If we find these are important features for the model then we can also come back and expand on the amount of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts5 = data['ward'].value_counts()\n",
    "verybig = counts5.loc[counts5.between(200,400)].index\n",
    "big = counts5.loc[counts5.between(100,200)].index\n",
    "medium = counts5.loc[counts5.between(50,100)].index\n",
    "small = counts5.loc[counts5.between(25,50)].index\n",
    "verysmall = counts5.loc[counts5 <=25].index\n",
    "data.loc[data['ward'].isin(verybig), 'ward'] = 'verybig'\n",
    "data.loc[data['ward'].isin(big), 'ward'] = 'big'\n",
    "data.loc[data['ward'].isin(medium), 'ward'] = 'medium'\n",
    "data.loc[data['ward'].isin(small), 'ward'] = 'small'\n",
    "data.loc[data['ward'].isin(verysmall), 'ward'] = 'verysmall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ward'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at a comparison of classes and sources\n",
    "Chi_sq_test(data, 'class', 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the latitude and longitude columns. It is possible we will see this ditribution line up well with population as we expect more wellpoints in populated areas, though we aren't sure what the calss trend for these areas is yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "sns.scatterplot(data=data, y='latitude', x='longitude', hue='class')\n",
    "plt.title('Latitude/Longitude Mapping')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a few datapoints that seem to have incorrect values, lets take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['longitude'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['longitude']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points all have the Lake Victoria basin. We will use an random value between the 25th and 75th percentiles from all datapoints in the Lake Victoria basin to fill in the zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['basin']=='Lake Victoria']['longitude'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['longitude'] == 0, 'longitude'] = np.random.choice(range(31,33), 1812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "sns.scatterplot(data=data, y='latitude', x='longitude', hue='class')\n",
    "plt.title('Latitude/Longitude Mapping')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still an issue with a latitude value being zero, lets take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding all columns with latitude == 0\n",
    "data.loc[data['latitude']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#widening the search\n",
    "data.loc[data['latitude']>-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can further tighen the search now that we see some of the issue values as -2.0E-08\n",
    "data.loc[data['latitude']>-.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill these values in a similar manner to the longitiude values using a random value between the 25th and 75th percentile of Lake Victoria basin datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['basin']=='Lake Victoria']['latitude'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['latitude']>-0.01, 'latitude'] = -1*np.random.choice(range(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "sns.scatterplot(data=data, y='latitude', x='longitude', hue='class')\n",
    "plt.title('Latitude/Longitude Mapping')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "sns.scatterplot(data=data, y='latitude', x='longitude', hue='gps_height')\n",
    "plt.title('Latitude/Longitude vs. GPS Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['gps_height'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['gps_height']>0, 'gps_height'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_gps_trainX = data.loc[data['gps_height']>0, ['latitude', 'longitude']]\n",
    "lin_gps_trainy = data.loc[data['gps_height']>0, ['gps_height']]\n",
    "lin = LinearRegression()\n",
    "lin.fit(lin_gps_trainX, lin_gps_trainy)\n",
    "data.loc[data['gps_height']<=0, ['gps_height']] = lin.predict(data.loc[data['gps_height']<=0, ['latitude', 'longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[data['gps_height']<1, 'gps_height'] = np.random.choice(range(471, 1512), 59400-37466)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(data=data, x='gps_height', hue='class', multiple='dodge', bins=15, element='poly', fill=False)\n",
    "plt.title('Class Distribution vs. GPS Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['amount_tsh'] = data['amount_tsh'].astype('int')\n",
    "data['amount_tsh'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['amount_tsh'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['amount_tsh']>2000, 'amount_tsh'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['amount_tsh'].sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to seperate this into bins to allow it function better with the classification algorithims. There also appears to be some issues with the top end values as 350,000 ft of head is more than 10x the height of Mt. Everest aka not possible for static head. We will use Standard Scaler on this column during feature engineering to ensure these high values don't cause the feature to be overweighted. We are also going to cap all values as 5000 and because more than two thirds of the column is at value zero, we will turn this into a categorical column of bins and eliminate the continuous column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['amount_tsh']>2000, 'amount_tsh'] = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['amount_tsh']=values['amount_tsh']\n",
    "data.loc[~data['amount_tsh'].between(1,4999), 'amount_tsh'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lin_tsh_trainX = data.loc[data['amount_tsh'].between(1,4999), ['gps_height', 'latitude', 'longitude']]\n",
    "#lin_tsh_trainy = data.loc[data['amount_tsh'].between(1,4999), ['amount_tsh']]\n",
    "lin_tsh = LinearRegression()\n",
    "lin_tsh.fit(lin_tsh_trainX, lin_tsh_trainy)\n",
    "lin_tsh.score(lin_tsh_trainX,lin_tsh_trainy)\n",
    "#data.loc[~data['amount_tsh'].between(1,4999), 'amount_tsh'] = lin_tsh.predict(data.loc[~data['amount_tsh'].between(1,4999), ['gps_height', 'latitude', 'longitude']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[~data['amount_tsh'].between(1,4999), 'amount_tsh'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data, x='gps_height', y='amount_tsh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tsh_trainX = data[['latitude', 'longitude']]\n",
    "scaler = StandardScaler()\n",
    "gps = scaler.fit_transform(data['gps_height'].values.reshape(-1,1))\n",
    "lin_tsh_trainX.insert(1, value=gps, column='gps_height')\n",
    "lin_tsh_trainX = lin_tsh_trainX.merge(pd.get_dummies(data[['waterpoint_type', 'quantity', 'extraction_type','source']]),\n",
    "                                      left_index=True, right_index=True)\n",
    "lin_tsh_trainX.insert(0, column = 'tsh', value=data['amount_tsh'])\n",
    "lin_tsh_trainX = lin_tsh_trainX.loc[lin_tsh_trainX['tsh'].between(1,4999)]\n",
    "lin_tsh_trainX.drop(columns=['tsh'], inplace=True)\n",
    "lin_tsh_trainy = data.loc[data['amount_tsh'].between(1,4999), ['amount_tsh']]\n",
    "lin_tsh_trainX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, tX, y, ty =  train_test_split(lin_tsh_trainX, lin_tsh_trainy, test_size=0.25, random_state=42)\n",
    "lin_tsh = ElasticNet(max_iter=5000)\n",
    "rsearch = RandomizedSearchCV(lin_tsh, param_distributions={'alpha':[.1, .2, .3, .4, .5, .6, .7, .8, .9, 1], \n",
    "                                                           'l1_ratio':[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1]},\n",
    "                            scoring= 'neg_mean_absolute_error', cv=4)#', 'neg_mean_absolute_percentage_error', \n",
    "                                     #'neg_root_mean_squared_error', 'r2'], refit='r2', cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsearch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsearch.score(tX, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rsearch.predict(tX)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tsh_fillX = data[['latitude', 'longitude']]\n",
    "scaler = StandardScaler()\n",
    "gps = scaler.fit_transform(data['gps_height'].values.reshape(-1,1))\n",
    "lin_tsh_fillX.insert(1, value=gps, column='gps_height')\n",
    "lin_tsh_fillX = lin_tsh_fillX.merge(pd.get_dummies(data[['waterpoint_type', 'quantity', 'extraction_type','source']]),\n",
    "                                      left_index=True, right_index=True)\n",
    "lin_tsh_fillX.insert(0, column = 'tsh', value=data['amount_tsh'])\n",
    "lin_tsh_fillX = lin_tsh_fillX.loc[~lin_tsh_trainX['tsh'].between(1,4999)]\n",
    "lin_tsh_trainy = data.loc[data['amount_tsh'].between(1,4999), ['amount_tsh']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsh_pred = rsearch.predict(lin_tsh_fillX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tsh_trainX.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tsh_bins'] = data['amount_tsh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tsh_bins'] = pd.qcut(data['tsh_bins'], q=5, duplicates='drop',labels=False)\n",
    "data['tsh_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['amount_tsh']=data['amount_tsh'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['amount_tsh'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see visually if the ratio of functional to non functional seems to improve based on higher tsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(data = data[['amount_tsh', 'class']], x='amount_tsh', hue='class', multiple='dodge')\n",
    "plt.title('Distribution of Classes by Static Head Pressure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not suprisingly the more head available the higher the percentage of wellpoints are functional. This makes sense when you consider that higher head pressures would be able to more easily push through mechancial issues like lack of lubrication as well as dirt build up in piping. We assuming that these are common cause of non functional class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['date_recorded', 'construction_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('Data/train_values_EDA2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have further EDA and cleaning that could be done we are going to move forward at this point to see how our model performs with what we currently have. We can always come back and ajust the number of bins for a category and or maxes and mins for continuos variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a single function that will perform all of our adjustments above. This will allow it to be run on the submission values as well and guarantee we are using the same process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Functions\\model_transformer.py \n",
    "def model_transformer(model_data):\n",
    "    #Applies transformations from EDA notebook to training and testing sets to ensure same changes are made\n",
    "    #Correct names in extraction_type\n",
    "    data = model_data\n",
    "    \n",
    "    data['extraction_type'].replace({'other - swn 81':'other-handpump',\n",
    "                                 'other - play pump':'other-handpump', \n",
    "                                 'walimi':'other-handpump', \n",
    "                                 'other - mkulima/shinyanga':'other-handpump',\n",
    "                                'swn 80':'swn_80',\n",
    "                                 'nira/tanira':'nira-tanira',\n",
    "                                'india mark ii':'india_mark_ii',\n",
    "                                'india mark iii':'india_mark_iii',\n",
    "                                'other - rope pump':'other-rope_pump',}, inplace=True)\n",
    "    #correct names in source\n",
    "    data['source'].replace({'shallow well':'shallow_well',\n",
    "                       'machine dbh':'machine_dbh',\n",
    "                       'rainwater harvesting':'rainwater_harvesting',\n",
    "                       'hand dtw':'hand_dtw'}, inplace=True)\n",
    "\n",
    "    #Group low count subvillages in other\n",
    "    counts = data['subvillage'].value_counts()\n",
    "    counts = counts.loc[counts >=200]\n",
    "    counts = list(counts.index)\n",
    "    data.loc[~data['subvillage'].isin(counts), 'subvillage'] = 'other'\n",
    "\n",
    "    data.fillna(inplace=True, value={'installer':'unknown','permit':False, 'funder':'unknown', 'public_meeting':False, \n",
    "                                 'scheme_management':'unknown', 'scheme_name':'unknown'})\n",
    "\n",
    "    #create and boolean lga_Njombe column\n",
    "    data['lga_Njombe'] = data['lga'].replace({'Njombe':1})\n",
    "    data.loc[data['lga_Njombe']!=1, 'lga_Njombe'] = 0\n",
    "    data['lga_Njombe'] = data['lga_Njombe'].astype('int')\n",
    "\n",
    "    #remove slashes from basin names\n",
    "    data['basin'].replace({'Ruvuma / Southern Coast':'Ruvuma-Southern_Coast',\n",
    "                     'Wami / Ruvu':'Wami-Ruvu'}, inplace=True)\n",
    "\n",
    "    #convert date_recorded column to datetime object and edxtract month and year\n",
    "    data['date_recorded']= pd.to_datetime(data['date_recorded'])\n",
    "    data['date_recorded'].describe(datetime_is_numeric=True)\n",
    "    data['year']=data['date_recorded'].dt.year\n",
    "    data['month']=data['date_recorded'].dt.month\n",
    "\n",
    "    ##Convert public_meeting column to 1 or 0\n",
    "    data['public_meeting'] = data['public_meeting'].map({True:1, False:0})\n",
    "\n",
    "    #Convert permit column to 1 or 0\n",
    "    data['permit'] = data['permit'].map({True:1, False:0})\n",
    "\n",
    "    #Correct construction_year with 1999, create years_old column\n",
    "    data['construction_year'].replace({0:1999}, inplace=True)\n",
    "    data['construction_year'] = pd.to_datetime(data['construction_year'], format='%Y')\n",
    "    data['years_old'] = data['date_recorded'].dt.year - data['construction_year'].dt.year\n",
    "\n",
    "    #Group low count scheme_names under other\n",
    "    counts2 = data['scheme_name'].value_counts()\n",
    "    counts2 = counts2.loc[counts2 >=200]\n",
    "    counts2 = list(counts2.index)\n",
    "    data.loc[~data['scheme_name'].isin(counts2), 'scheme_name'] = 'other'\n",
    "    \n",
    "    #group low count funders under other\n",
    "    counts3 = data['funder'].value_counts()\n",
    "    counts3 = counts3.loc[counts3 >=500]\n",
    "    counts3 = list(counts3.index)\n",
    "    data.loc[~data['funder'].isin(counts3), 'funder'] = 'other'\n",
    "    data.loc[data['funder']=='Government Of Tanzania', 'funder'] = 'gov_tanz'\n",
    "\n",
    "    #Group low count installers under other\n",
    "    counts4 = data['installer'].value_counts()\n",
    "    counts4 = counts4.loc[counts4 >=500]\n",
    "    counts4 = list(counts4.index)\n",
    "    data.loc[~data['installer'].isin(counts4), 'installer'] = 'other'\n",
    "    \n",
    "    #Create column for population bins\n",
    "    data['popbins'] = pd.cut(data['population'], [-1,2,250,500,1000,2500,10000,40000], labels=list(range(1,8)))\n",
    "    \n",
    "    #Amount_TSH - Change to bins\n",
    "    data.loc[data['amount_tsh']>5000, 'amount_tsh'] = 5000\n",
    "    data.loc[data['amount_tsh']>0, 'amount_tsh'] = pd.qcut(data.loc[data['amount_tsh']>0, 'amount_tsh'], \n",
    "                                                           q=5, duplicates='drop',labels=False)           \n",
    "    \n",
    "    #Ward Feature - Change to Bins\n",
    "    counts5 = data['ward'].value_counts()\n",
    "    verybig = counts5.loc[counts5.between(200,400)].index\n",
    "    big = counts5.loc[counts5.between(100,200)].index\n",
    "    medium = counts5.loc[counts5.between(50,100)].index\n",
    "    small = counts5.loc[counts5.between(25,50)].index\n",
    "    verysmall = counts5.loc[counts5 <=25].index\n",
    "    data.loc[data['ward'].isin(verybig), 'ward'] = 'verybig'\n",
    "    data.loc[data['ward'].isin(big), 'ward'] = 'big'\n",
    "    data.loc[data['ward'].isin(medium), 'ward'] = 'medium'\n",
    "    data.loc[data['ward'].isin(small), 'ward'] = 'small'\n",
    "    data.loc[data['ward'].isin(verysmall), 'ward'] = 'verysmall'\n",
    "    \n",
    "    #Latitude-Longitiude - Correct near zero values\n",
    "    data.loc[data['longitude'] == 0, 'longitude'] = np.random.choice(range(31,33))\n",
    "    data.loc[data['latitude']>-0.01, 'latitude'] = -1*np.random.choice(range(1,2))\n",
    "    \n",
    "    lin_gps_trainX = data.loc[data['gps_height']>0, ['latitude', 'longitude']]\n",
    "    lin_gps_trainy = data.loc[data['gps_height']>0, ['gps_height']]\n",
    "    lin = LinearRegression()\n",
    "    lin.fit(lin_gps_trainX, lin_gps_trainy)\n",
    "    data.loc[data['gps_height']<=0, ['gps_height']] = lin.predict(data.loc[data['gps_height']<=0, ['latitude', 'longitude']])\n",
    "    \n",
    "    data.drop(columns=['source_type', 'source_class', 'extraction_type_group', 'extraction_type_class', \n",
    "                       'region', 'wpt_name', 'num_private', 'recorded_by', 'quality_group', 'quantity_group',\n",
    "                       'waterpoint_type_group', 'payment', 'construction_year', 'date_recorded', 'lga'], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_transformer(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns == model_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.to_pickle('Data/model_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pumpitup",
   "language": "python",
   "name": "pumpitup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
