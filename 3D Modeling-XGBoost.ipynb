{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in standard packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in ML packages/modules\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Scheduler: \"tcp://127.0.0.1:60181\" processes: 3 cores: 6>,\n",
       " {0: <Nanny: tcp://127.0.0.1:60201, threads: 2>,\n",
       "  1: <Nanny: tcp://127.0.0.1:60198, threads: 2>,\n",
       "  2: <Nanny: tcp://127.0.0.1:60204, threads: 2>})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in dask package and set up to allow for faster compute times. Distributes computing among all availabel preocessors\n",
    "from dask import dataframe as dd\n",
    "import joblib\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "cluster.scheduler, cluster.workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in train/test data\n",
    "train_X = pd.read_pickle('Data/train_test/train_X.pkl')\n",
    "test_X = pd.read_pickle('Data/train_test/test_X.pkl')\n",
    "train_y = pd.read_pickle('Data/train_test/train_y.pkl')\n",
    "test_y = pd.read_pickle('Data/train_test/test_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing classes to numeric values for ingestion into model\n",
    "train_y = train_y.map({'functional':2, 'non functional':0, 'functional needs repair':1})\n",
    "test_y = test_y.map({'functional':2, 'non functional':0, 'functional needs repair':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7917171717171717"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First we will try an out of the box implemenetation\n",
    "xgb = XGBClassifier(verbosity=1, eval_metric='mlogloss', use_label_encoder=False, objective='softmax')\n",
    "with joblib.parallel_backend('dask'):\n",
    "    xgb.fit(train_X, train_y)\n",
    "xgb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs pretty well. We can further tune some hyperparamters to possibly get higher accuracy. A tree booster was used for this out of the box model, let's try a linear booster now. But we need to first save the parameters and score for later review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>objective</th>\n",
       "      <th>use_label_encoder</th>\n",
       "      <th>base_score</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bynode</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>gpu_id</th>\n",
       "      <th>...</th>\n",
       "      <th>num_parallel_tree</th>\n",
       "      <th>random_state</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "      <th>validate_parameters</th>\n",
       "      <th>verbosity</th>\n",
       "      <th>eval_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGB_OBO</th>\n",
       "      <td>0.791717</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Accuracy       objective  use_label_encoder  base_score booster  \\\n",
       "XGB_OBO  0.791717  multi:softprob              False         0.5  gbtree   \n",
       "\n",
       "         colsample_bylevel  colsample_bynode  colsample_bytree  gamma  gpu_id  \\\n",
       "XGB_OBO                  1                 1                 1      0      -1   \n",
       "\n",
       "         ... num_parallel_tree random_state  reg_alpha  reg_lambda  \\\n",
       "XGB_OBO  ...                 1            0          0           1   \n",
       "\n",
       "         scale_pos_weight  subsample  tree_method validate_parameters  \\\n",
       "XGB_OBO              None          1        exact                   1   \n",
       "\n",
       "         verbosity  eval_metric  \n",
       "XGB_OBO          1     mlogloss  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dataframe to store metrics and parameters\n",
    "xgb_deets = pd.DataFrame(xgb.get_params(), index=['XGB_OBO'])\n",
    "xgb_deets.insert(0, value=xgb.score(test_X, test_y), column='Accuracy')\n",
    "xgb_deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7518518518518519"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUn model with linear booster\n",
    "xgb_lin = XGBClassifier( verbosity=1, eval_metric='mlogloss', booster='gblinear', \n",
    "                         use_label_encoder=False, objective='softmax')\n",
    "with joblib.parallel_backend('dask'):\n",
    "    xgb_lin.fit(train_X, train_y)\n",
    "xgb_lin.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible, but much worse than the tree booster, we will stick with that going forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>objective</th>\n",
       "      <th>use_label_encoder</th>\n",
       "      <th>base_score</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bynode</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>gpu_id</th>\n",
       "      <th>...</th>\n",
       "      <th>num_parallel_tree</th>\n",
       "      <th>random_state</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "      <th>validate_parameters</th>\n",
       "      <th>verbosity</th>\n",
       "      <th>eval_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGB_OBO</th>\n",
       "      <td>0.791717</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_LIN</th>\n",
       "      <td>0.751852</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Accuracy       objective  use_label_encoder  base_score   booster  \\\n",
       "XGB_OBO  0.791717  multi:softprob              False         0.5    gbtree   \n",
       "XGB_LIN  0.751852  multi:softprob              False         0.5  gblinear   \n",
       "\n",
       "        colsample_bylevel colsample_bynode colsample_bytree gamma  gpu_id  \\\n",
       "XGB_OBO                 1                1                1     0      -1   \n",
       "XGB_LIN              None             None             None  None      -1   \n",
       "\n",
       "         ... num_parallel_tree random_state  reg_alpha reg_lambda  \\\n",
       "XGB_OBO  ...                 1            0          0          1   \n",
       "XGB_LIN  ...              None            0          0          0   \n",
       "\n",
       "        scale_pos_weight subsample  tree_method validate_parameters  \\\n",
       "XGB_OBO             None         1        exact                   1   \n",
       "XGB_LIN             None      None         None                   1   \n",
       "\n",
       "         verbosity  eval_metric  \n",
       "XGB_OBO          1     mlogloss  \n",
       "XGB_LIN          1     mlogloss  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store metrics and parameters\n",
    "a = xgb_lin.get_params()\n",
    "a['Accuracy'] = xgb_lin.score(test_X, test_y)\n",
    "xgb_deets = pd.concat([xgb_deets, pd.DataFrame(a, index=['XGB_LIN'])], axis=0, join='outer')\n",
    "xgb_deets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tree booster works better than the linear so we will stick with that. XGBoost provides recommendations for unbalanced datasets: setting max_delta_step to an integer between 1 and 10 and setting scale_pos_weight to the ratio of labels (we will use functional as positive and the other two as negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.067112288398659"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating our ratio, 'functional' is positive here and both 'non functional' and 'functional needs repair' are negative\n",
    "ratio = train_y.loc[train_y!=2].sum()/train_y.loc[train_y==2].sum()\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:13:22] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:14:25] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:15:34] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:16:39] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:17:44] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:18:54] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:19:58] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:21:07] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>objective</th>\n",
       "      <th>use_label_encoder</th>\n",
       "      <th>base_score</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bynode</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>gpu_id</th>\n",
       "      <th>...</th>\n",
       "      <th>num_parallel_tree</th>\n",
       "      <th>random_state</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "      <th>validate_parameters</th>\n",
       "      <th>verbosity</th>\n",
       "      <th>eval_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGB_OBO</th>\n",
       "      <td>0.791717</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_LIN</th>\n",
       "      <td>0.751852</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gblinear</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_1_1</th>\n",
       "      <td>0.794209</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_0.067112288398659_1</th>\n",
       "      <td>0.794209</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067112</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_1_3</th>\n",
       "      <td>0.790572</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_0.067112288398659_3</th>\n",
       "      <td>0.790572</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067112</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_1_7</th>\n",
       "      <td>0.791717</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_0.067112288398659_7</th>\n",
       "      <td>0.791717</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067112</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_1_10</th>\n",
       "      <td>0.791717</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_0.067112288398659_10</th>\n",
       "      <td>0.791717</td>\n",
       "      <td>multi:softprob</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>gbtree</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067112</td>\n",
       "      <td>1</td>\n",
       "      <td>exact</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mlogloss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy       objective  use_label_encoder  \\\n",
       "XGB_OBO                   0.791717  multi:softprob              False   \n",
       "XGB_LIN                   0.751852  multi:softprob              False   \n",
       "XGB_1_1                   0.794209  multi:softprob              False   \n",
       "XGB_0.067112288398659_1   0.794209  multi:softprob              False   \n",
       "XGB_1_3                   0.790572  multi:softprob              False   \n",
       "XGB_0.067112288398659_3   0.790572  multi:softprob              False   \n",
       "XGB_1_7                   0.791717  multi:softprob              False   \n",
       "XGB_0.067112288398659_7   0.791717  multi:softprob              False   \n",
       "XGB_1_10                  0.791717  multi:softprob              False   \n",
       "XGB_0.067112288398659_10  0.791717  multi:softprob              False   \n",
       "\n",
       "                          base_score   booster colsample_bylevel  \\\n",
       "XGB_OBO                          0.5    gbtree                 1   \n",
       "XGB_LIN                          0.5  gblinear              None   \n",
       "XGB_1_1                          0.5    gbtree                 1   \n",
       "XGB_0.067112288398659_1          0.5    gbtree                 1   \n",
       "XGB_1_3                          0.5    gbtree                 1   \n",
       "XGB_0.067112288398659_3          0.5    gbtree                 1   \n",
       "XGB_1_7                          0.5    gbtree                 1   \n",
       "XGB_0.067112288398659_7          0.5    gbtree                 1   \n",
       "XGB_1_10                         0.5    gbtree                 1   \n",
       "XGB_0.067112288398659_10         0.5    gbtree                 1   \n",
       "\n",
       "                         colsample_bynode colsample_bytree gamma  gpu_id  ...  \\\n",
       "XGB_OBO                                 1                1     0      -1  ...   \n",
       "XGB_LIN                              None             None  None      -1  ...   \n",
       "XGB_1_1                                 1                1     0      -1  ...   \n",
       "XGB_0.067112288398659_1                 1                1     0      -1  ...   \n",
       "XGB_1_3                                 1                1     0      -1  ...   \n",
       "XGB_0.067112288398659_3                 1                1     0      -1  ...   \n",
       "XGB_1_7                                 1                1     0      -1  ...   \n",
       "XGB_0.067112288398659_7                 1                1     0      -1  ...   \n",
       "XGB_1_10                                1                1     0      -1  ...   \n",
       "XGB_0.067112288398659_10                1                1     0      -1  ...   \n",
       "\n",
       "                         num_parallel_tree random_state  reg_alpha reg_lambda  \\\n",
       "XGB_OBO                                  1            0          0          1   \n",
       "XGB_LIN                               None            0          0          0   \n",
       "XGB_1_1                                  1            0          0          1   \n",
       "XGB_0.067112288398659_1                  1            0          0          1   \n",
       "XGB_1_3                                  1            0          0          1   \n",
       "XGB_0.067112288398659_3                  1            0          0          1   \n",
       "XGB_1_7                                  1            0          0          1   \n",
       "XGB_0.067112288398659_7                  1            0          0          1   \n",
       "XGB_1_10                                 1            0          0          1   \n",
       "XGB_0.067112288398659_10                 1            0          0          1   \n",
       "\n",
       "                         scale_pos_weight subsample  tree_method  \\\n",
       "XGB_OBO                              None         1        exact   \n",
       "XGB_LIN                              None      None         None   \n",
       "XGB_1_1                                 1         1        exact   \n",
       "XGB_0.067112288398659_1          0.067112         1        exact   \n",
       "XGB_1_3                                 1         1        exact   \n",
       "XGB_0.067112288398659_3          0.067112         1        exact   \n",
       "XGB_1_7                                 1         1        exact   \n",
       "XGB_0.067112288398659_7          0.067112         1        exact   \n",
       "XGB_1_10                                1         1        exact   \n",
       "XGB_0.067112288398659_10         0.067112         1        exact   \n",
       "\n",
       "                         validate_parameters  verbosity  eval_metric  \n",
       "XGB_OBO                                    1          1     mlogloss  \n",
       "XGB_LIN                                    1          1     mlogloss  \n",
       "XGB_1_1                                    1          1     mlogloss  \n",
       "XGB_0.067112288398659_1                    1          1     mlogloss  \n",
       "XGB_1_3                                    1          1     mlogloss  \n",
       "XGB_0.067112288398659_3                    1          1     mlogloss  \n",
       "XGB_1_7                                    1          1     mlogloss  \n",
       "XGB_0.067112288398659_7                    1          1     mlogloss  \n",
       "XGB_1_10                                   1          1     mlogloss  \n",
       "XGB_0.067112288398659_10                   1          1     mlogloss  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run through for loop for hyperparameter tuning\n",
    "step = [1, 3, 7, 10]\n",
    "weight = [1, ratio]\n",
    "for i in step:\n",
    "    for r in weight:\n",
    "        xgb_opts = XGBClassifier(scale_pos_weight=r, verbosity=1, eval_metric='mlogloss', \n",
    "                                  use_label_encoder=False, max_delta_step=i)\n",
    "        with joblib.parallel_backend('dask'):\n",
    "            xgb_opts.fit(train_X, train_y)\n",
    "        xgb_opts.score(test_X, test_y)\n",
    "        deets = xgb_opts.get_params()\n",
    "        deets['Accuracy']=xgb_opts.score(test_X, test_y)\n",
    "        xgb_deets = pd.concat([xgb_deets, pd.DataFrame(deets, index=['XGB_{r}_{i}'.format(r=r, i=i)])], axis=0, join='outer')\n",
    "xgb_deets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of class weights does not seem to affect the accuracy. The max_delta_step does affect it, but only shows that using a value below ten reduces accuracy from the out of the box model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:22:13] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7917171717171717"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-run best model\n",
    "xgb_f = XGBClassifier(scale_pos_weight=1, verbosity=1, eval_metric='mlogloss', \n",
    "                                  use_label_encoder=False, max_delta_step=10)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    xgb_f.fit(train_X, train_y)\n",
    "xgb_f.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2880x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.72      0.78      5678\n",
      "           1       0.64      0.25      0.36      1074\n",
      "           2       0.77      0.91      0.84      8098\n",
      "\n",
      "    accuracy                           0.79     14850\n",
      "   macro avg       0.75      0.63      0.66     14850\n",
      "weighted avg       0.79      0.79      0.78     14850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Visualize metrics by class using Confusion martix and Classification Report\n",
    "y_pred = xgb_f.predict(test_X)\n",
    "cm=confusion_matrix(test_y, y_pred, labels=xgb_f.classes_)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=xgb_f.classes_)\n",
    "plt.figure(figsize=(40,8))\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "print(classification_report(test_y, y_pred, labels=xgb_f.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is consistent with what we seeing from other models, the functional needs repair class accuracy is holding back the overall accuracy. XGBoost is below the other tree based models on this class, but still above the linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:23:21] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:24:14] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:25:06] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:25:56] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:26:51] WARNING: ..\\src\\learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0.79427609 0.79539843 0.79057239 0.79842873 0.78540965]\n"
     ]
    }
   ],
   "source": [
    "#Confirm generalizability via 5-fold cross validation\n",
    "print(cross_val_score(xgb_f, train_X, train_y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output for comparision\n",
    "xgb_deets.to_pickle('Data/scores/XGB.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pumpitup",
   "language": "python",
   "name": "pumpitup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
